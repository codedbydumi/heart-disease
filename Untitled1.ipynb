{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e263070-be56-4f4e-b504-2662e3467c1c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## üîç 1. `RandomizedSearchCV` vs `GridSearchCV`\n",
    "\n",
    "Both are used for **hyperparameter tuning** (finding the best settings for your model).\n",
    "\n",
    "* **GridSearchCV**\n",
    "\n",
    "  * Tries **every possible combination** of hyperparameters you give.\n",
    "  * Example: If you test `n_estimators = [100, 200]` and `max_depth = [5, 10]`, it will try all 4 combinations.\n",
    "  * ‚úÖ More thorough\n",
    "  * ‚ùå Slower, especially with many parameters\n",
    "\n",
    "* **RandomizedSearchCV**\n",
    "\n",
    "  * Instead of trying everything, it picks **random combinations** of hyperparameters (but still intelligently).\n",
    "  * Example: If you allow 100 possible values, it might test only 20 randomly.\n",
    "  * ‚úÖ Faster, works well for large search spaces\n",
    "  * ‚ùå Might miss the absolute best one\n",
    "\n",
    "üí° Analogy:\n",
    "\n",
    "* GridSearch = ‚ÄúTry every dish on the menu to see the best.‚Äù\n",
    "* RandomizedSearch = ‚ÄúPick random dishes, but still get something tasty faster.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 2. `confusion_matrix`\n",
    "\n",
    "* A table that shows **how well your classification model did**.\n",
    "* It compares predictions vs actual labels.\n",
    "\n",
    "Example for binary classification (Spam/Not Spam):\n",
    "\n",
    "|                     | Predicted Spam      | Predicted Not Spam  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Spam**     | True Positive (TP)  | False Negative (FN) |\n",
    "| **Actual Not Spam** | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "* **TP**: Correctly predicted spam\n",
    "* **TN**: Correctly predicted not spam\n",
    "* **FP**: Predicted spam but it wasn‚Äôt\n",
    "* **FN**: Predicted not spam but it was\n",
    "\n",
    "---\n",
    "\n",
    "## üìù 3. `classification_report`\n",
    "\n",
    "* A **summary of precision, recall, f1-score** for each class.\n",
    "* Easier than manually calculating metrics.\n",
    "\n",
    "Output looks like:\n",
    "\n",
    "```\n",
    "              precision   recall   f1-score   support\n",
    "Class 0          0.85      0.90      0.87      100\n",
    "Class 1          0.78      0.70      0.74       50\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ 4. `precision_score`\n",
    "\n",
    "* Out of all **predicted positives**, how many were correct?\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "‚úÖ Good when **false positives are costly**.\n",
    "Example: Predicting if an email is spam ‚Üí we don‚Äôt want to wrongly classify important emails as spam.\n",
    "\n",
    "---\n",
    "\n",
    "## üì° 5. `recall_score`\n",
    "\n",
    "* Out of all **actual positives**, how many did we correctly find?\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "‚úÖ Good when **false negatives are costly**.\n",
    "Example: Cancer detection ‚Üí missing a positive case is very dangerous.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è 6. `f1_score`\n",
    "\n",
    "* The **balance between precision and recall**.\n",
    "* Harmonic mean of precision & recall.\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "‚úÖ Useful when you need balance (like fraud detection).\n",
    "\n",
    "---\n",
    "\n",
    "## üìà 7. `RocCurveDisplay`\n",
    "\n",
    "* Plots the **ROC Curve (Receiver Operating Characteristic Curve)**.\n",
    "* Shows trade-off between **True Positive Rate (recall)** and **False Positive Rate** at different thresholds.\n",
    "* Also gives **AUC (Area Under Curve)**:\n",
    "\n",
    "  * 0.5 = random guessing\n",
    "  * 1.0 = perfect model\n",
    "\n",
    "üí° Example: In medical tests, you may adjust the threshold:\n",
    "\n",
    "* Lower threshold ‚Üí catch more sick people (high recall) but risk more false alarms.\n",
    "* Higher threshold ‚Üí fewer false alarms but miss some sick people.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Quick summary table**\n",
    "\n",
    "| Tool                   | What it does                        | Use case                        |\n",
    "| ---------------------- | ----------------------------------- | ------------------------------- |\n",
    "| RandomizedSearchCV     | Random hyperparameter search        | Fast tuning                     |\n",
    "| GridSearchCV           | Exhaustive hyperparameter search    | Best when search space is small |\n",
    "| confusion\\_matrix      | Counts TP, FP, TN, FN               | Detailed performance            |\n",
    "| classification\\_report | Summary (precision, recall, f1)     | Quick overview                  |\n",
    "| precision\\_score       | Correctness of positive predictions | When false positives are bad    |\n",
    "| recall\\_score          | How many actual positives we found  | When false negatives are bad    |\n",
    "| f1\\_score              | Balance between precision & recall  | General performance             |\n",
    "| RocCurveDisplay        | Curve of TPR vs FPR                 | Visual threshold trade-off      |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b911252-6d80-471b-ab83-979b3287cada",
   "metadata": {},
   "source": [
    "Perfect üëç You‚Äôre diving into **classification models** now. Let me explain each of the three models you imported in **very simple, beginner-friendly terms** so it sticks.\n",
    "\n",
    "## üîπ 1. Logistic Regression\n",
    "\n",
    "‚û°Ô∏è Despite the name, **Logistic Regression is used for classification, not regression**.\n",
    "\n",
    "* **What it does**:\n",
    "  It learns the probability that something belongs to a class (e.g., spam or not spam).\n",
    "  If the probability > 0.5 ‚Üí predict class 1, else ‚Üí class 0.\n",
    "\n",
    "* **How it works (basic idea)**:\n",
    "\n",
    "  * It fits a line (or curve in higher dimensions) that best separates the classes.\n",
    "  * Uses the **sigmoid function (S-shaped curve)** to squash numbers between 0 and 1 ‚Üí probabilities.\n",
    "\n",
    "* **When to use**:\n",
    "\n",
    "  * Binary classification (yes/no, 0/1 problems).\n",
    "  * Easy to interpret and fast.\n",
    "\n",
    "‚úÖ Example: Predict whether a patient has diabetes or not.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. K-Nearest Neighbors (KNN)\n",
    "\n",
    "‚û°Ô∏è KNN is like \"asking your neighbors for advice.\"\n",
    "\n",
    "* **What it does**:\n",
    "  For a new data point, it looks at the **k closest points** (neighbors) in the training data.\n",
    "\n",
    "  * If most neighbors are Class A ‚Üí predict Class A.\n",
    "  * If most neighbors are Class B ‚Üí predict Class B.\n",
    "\n",
    "* **How it works (basic idea)**:\n",
    "\n",
    "  * You choose a number `k` (like 3 or 5).\n",
    "  * It uses distance (like Euclidean distance) to find the nearest neighbors.\n",
    "  * Votes among them to decide the class.\n",
    "\n",
    "* **When to use**:\n",
    "\n",
    "  * Works well with small to medium datasets.\n",
    "  * Not great with very large datasets (too slow).\n",
    "\n",
    "‚úÖ Example: Predict whether a fruit is an apple or orange based on its weight and color by looking at its closest fruits.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. Random Forest Classifier\n",
    "\n",
    "‚û°Ô∏è Random Forest is an **ensemble model** (uses many models together).\n",
    "\n",
    "* **What it does**:\n",
    "  It builds **lots of decision trees** (like yes/no flowcharts) and combines their results.\n",
    "  Each tree gives a \"vote,\" and the forest chooses the majority vote.\n",
    "\n",
    "* **How it works (basic idea)**:\n",
    "\n",
    "  * Randomly selects subsets of data and features.\n",
    "  * Builds many decision trees.\n",
    "  * Averages the predictions (for regression) or takes majority vote (for classification).\n",
    "\n",
    "* **Why it‚Äôs powerful**:\n",
    "\n",
    "  * Handles missing values and outliers well.\n",
    "  * Reduces overfitting compared to a single decision tree.\n",
    "\n",
    "‚úÖ Example: Predict whether a loan will default based on income, credit score, and employment.\n",
    "\n",
    "---\n",
    "\n",
    "üìä **Quick comparison**\n",
    "\n",
    "| Model               | Easy Explanation                 | Pros                       | Cons                   |\n",
    "| ------------------- | -------------------------------- | -------------------------- | ---------------------- |\n",
    "| Logistic Regression | Draws a line to separate classes | Fast, interpretable        | Only linear boundaries |\n",
    "| KNN                 | Looks at neighbors‚Äô classes      | Simple, no training needed | Slow on large data     |\n",
    "| Random Forest       | Many decision trees voting       | Very accurate, robust      | Harder to interpret    |\n",
    "\n",
    "---\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5414cf6-5940-4230-944c-d94e86c28393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d18b0034-640e-4681-8031-1276453e8608",
   "metadata": {},
   "source": [
    "Great question üëç ‚Äî **EDA** stands for **Exploratory Data Analysis**.\n",
    "\n",
    "It‚Äôs basically the **first step in data science or machine learning** where you **explore, clean, and understand your dataset** before building any model.\n",
    "\n",
    "---\n",
    "\n",
    "## üîé What is EDA?\n",
    "\n",
    "EDA = **Exploring the data to find patterns, trends, and insights.**\n",
    "Think of it as **‚Äúgetting to know your data‚Äù**.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Steps in EDA (basics)\n",
    "\n",
    "1. **Load the data**\n",
    "\n",
    "   * Use `pandas.read_csv()` or similar.\n",
    "\n",
    "2. **Look at the structure**\n",
    "\n",
    "   * `.head()`, `.info()`, `.describe()` ‚Üí check columns, types, missing values.\n",
    "\n",
    "3. **Check missing data**\n",
    "\n",
    "   * Which columns have `NaN`?\n",
    "   * Do we need to drop them or fill them?\n",
    "\n",
    "4. **Summary statistics**\n",
    "\n",
    "   * Mean, median, min, max, standard deviation.\n",
    "\n",
    "5. **Visualizations**\n",
    "\n",
    "   * **Univariate** (one variable): histograms, boxplots.\n",
    "   * **Bivariate** (two variables): scatter plots, bar plots.\n",
    "   * **Multivariate**: heatmaps, pairplots.\n",
    "\n",
    "6. **Correlation analysis**\n",
    "\n",
    "   * Use `df.corr()` + heatmap to see how features relate.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Example\n",
    "\n",
    "Say you have a dataset of **car sales**.\n",
    "\n",
    "* **Step 1**: Check how many rows and columns\n",
    "* **Step 2**: See missing values in \"Price\" or \"Odometer\"\n",
    "* **Step 3**: Plot histogram of car prices ‚Üí see distribution\n",
    "* **Step 4**: Compare \"Price\" vs \"Odometer\" ‚Üí maybe higher KM cars are cheaper\n",
    "* **Step 5**: Correlation heatmap ‚Üí check which features relate most with Price\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why is EDA important?\n",
    "\n",
    "* Helps you **understand the problem better**\n",
    "* Tells you if data needs **cleaning or transformation**\n",
    "* Guides you on **which features matter most**\n",
    "* Prevents mistakes (like using wrong column types)\n",
    "\n",
    "üí° Analogy:\n",
    "EDA is like **scouting the land before building a house** ‚Äî you check the soil, measure the area, and see the surroundings before construction.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3daeb-a16c-4b93-8865-c84701be2896",
   "metadata": {},
   "source": [
    "Great step üöÄ ‚Äî you‚Äôre now checking **correlation** in your dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ What `df.corr()` does\n",
    "\n",
    "* Looks at **numeric columns only** in your DataFrame.\n",
    "* Calculates pairwise **correlation coefficients** between them.\n",
    "* By default, it uses **Pearson correlation** (linear relationship).\n",
    "* Values range from **-1 ‚Üí +1**:\n",
    "\n",
    "  * `+1` = strong positive relationship (both go up together).\n",
    "  * `-1` = strong negative relationship (one goes up, the other goes down).\n",
    "  * `0` = no linear relationship.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example\n",
    "\n",
    "```python\n",
    "# correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "print(corr_matrix)\n",
    "```\n",
    "\n",
    "You‚Äôll get a table like:\n",
    "\n",
    "|          | age   | trestbps | chol  | thalach | target |\n",
    "| -------- | ----- | -------- | ----- | ------- | ------ |\n",
    "| age      | 1.00  | 0.28     | 0.21  | -0.40   | -0.22  |\n",
    "| trestbps | 0.28  | 1.00     | 0.30  | -0.15   | -0.05  |\n",
    "| chol     | 0.21  | 0.30     | 1.00  | -0.13   | -0.08  |\n",
    "| thalach  | -0.40 | -0.15    | -0.13 | 1.00    | 0.42   |\n",
    "| target   | -0.22 | -0.05    | -0.08 | 0.42    | 1.00   |\n",
    "\n",
    "Here you can see for example:\n",
    "\n",
    "* `thalach` (max heart rate) has **positive correlation** with `target` (0.42) ‚Üí higher heart rate = more chance of disease.\n",
    "* `age` has a **negative correlation** with `thalach` (-0.40).\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Visualizing with Seaborn\n",
    "\n",
    "Numbers alone are hard to read, so plot a **heatmap**:\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Heart Disease Dataset\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This will color-code the correlations for easier spotting.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44eecb-4335-4483-9f68-c0437f078496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3d919-a1db-43ff-8107-1a2a4669d774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
